# Vehicle CO₂ Emissions Analysis (Pandas + SQLite + Dash)

An end-to-end data project that cleans and analyzes a CO₂ emissions dataset, designs a normalized SQLite database, runs SQL-driven EDA, builds ML models (Random Forest & Gradient Boosting) to predict emissions, and serves an interactive dashboard with Dash.

---

## Table of Contents

- [Repository Structure](#repository-structure)  
- [Prerequisites](#prerequisites)  
- [Installation](#installation)  
- [Dataset](#dataset)  
- [Pipeline Overview](#pipeline-overview)  
- [Running the Analysis](#running-the-analysis)  
- [Database Schema & Sample Queries](#database-schema--sample-queries)  
- [Exploratory Data Analysis (SQL + Python)](#exploratory-data-analysis-sql--python)  
- [Machine Learning](#machine-learning)  
- [Interactive Dashboard](#interactive-dashboard)  
- [Results & Artifacts](#results--artifacts)  


---

## Repository Structure

```
├── co2.csv                           # Input dataset (source file you provide)
├── analysis.py                       # End-to-end script: clean → DB → EDA → ML
├── dashboard_app.py                  # Dash app for interactive exploration
├── requirements.txt                  # Python dependencies
├── vehicle_emissions_normalised.db   # SQLite DB (generated by analysis.py)
└── README.md
```



---

## Prerequisites

- Python 3.9+  
- Git  
- (Optional) A modern browser for the dashboard  
- (Optional) A CUDA-capable GPU (not required)

---

## Installation

1) **Clone the repo**
```bash
git clone https://github.com/your-org/vehicle-co2-analysis.git
cd vehicle-co2-analysis
```

2) **Create & activate a virtual environment**
```bash
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows (PowerShell)
.venv\Scripts\Activate.ps1
```

3) **Install dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```


---

## Dataset

Place your CSV at the repo root as `co2.csv`.  
Required columns (as used by the code):

- Categorical: `Make`, `Model`, `Vehicle Class`, `Transmission`, `Fuel Type`  
- Numeric: `Engine Size(L)`, `Cylinders`,  
  `Fuel Consumption City (L/100 km)`, `Fuel Consumption Hwy (L/100 km)`,  
  `Fuel Consumption Comb (L/100 km)`, `Fuel Consumption Comb (mpg)`,  
  `CO2 Emissions(g/km)`

---

## Pipeline Overview

1. **Data Wrangling & Cleaning**
   - Inspect schema & missingness
   - Drop duplicates
   - Outlier filtering (IQR & Z-score)
   - Feature engineering: `Fuel Efficiency Ratio = Fuel Consumption Comb (L/100 km) / Engine Size(L)`

2. **Relational DB Design (SQLite)**
   - Normalized tables:
     - `vehicles (vehicle_id, make, model, vehicle_class)`
     - `engine_specs (engine_id, vehicle_id, engine_size, cylinders, fuel_type)`
     - `fuel_emissions (fuel_id, vehicle_id, fuel_consumption_city, fuel_consumption_hwy, fuel_consumption_comb, fuel_consumption_comb_mpg, co2_emissions, fuel_efficiency)`
   - Load data from cleaned Pandas DataFrames → SQLite

3. **EDA via SQL + Python**
   - Top emitters / top fuel consumers
   - Distributions & scatter plots
   - Grouped aggregates (e.g., by vehicle class)
   - Correlation heatmap

4. **Machine Learning**
   - Random Forest Regressor (emissions prediction)
   - Gradient Boosting Regressor (feature importances)

5. **Interactive Dashboard (Dash)**
   - Dynamic scatter plot with categorical coloring
   - KPI tiles (avg CO₂, total vehicles, unique brands)
   - CO₂ range slider and variable selectors

---

## Running the Analysis

> Make sure `co2.csv` is present.

**Option A — single script**
```bash
python analysis.py
```
This will:
- Load & clean data
- Build `vehicle_emissions_normalised.db`
- Run example SQL queries and show result previews
- Produce plots (matplotlib/seaborn)
- Train/evaluate ML models and print metrics

**Option B — notebook**
If you kept this as a notebook, run all cells in order to reproduce the pipeline.

---

## Database Schema & Sample Queries

**Schema (created by the script):**
```sql
CREATE TABLE vehicles(
  vehicle_id   INTEGER PRIMARY KEY,
  make         TEXT,
  model        TEXT,
  vehicle_class TEXT
);

CREATE TABLE engine_specs(
  engine_id    INTEGER PRIMARY KEY AUTOINCREMENT,
  vehicle_id   INTEGER,
  engine_size  REAL,
  cylinders    INTEGER,
  fuel_type    TEXT,
  FOREIGN KEY (vehicle_id) REFERENCES vehicles(vehicle_id)
);

CREATE TABLE fuel_emissions(
  fuel_id                    INTEGER PRIMARY KEY AUTOINCREMENT,
  vehicle_id                 INTEGER,
  fuel_consumption_city      REAL,
  fuel_consumption_hwy       REAL,
  fuel_consumption_comb      REAL,
  fuel_consumption_comb_mpg  INTEGER,
  co2_emissions              INTEGER,
  fuel_efficiency            INTEGER,
  FOREIGN KEY (vehicle_id) REFERENCES vehicles(vehicle_id)
);
```

**Examples:**
```sql
-- Top 10 by combined fuel consumption
SELECT v.make, v.model, f."Fuel Consumption Comb (L/100 km)"
FROM vehicles v
JOIN fuel_emissions f ON v.vehicle_id = f.vehicle_id
ORDER BY f."Fuel Consumption Comb (L/100 km)" DESC
LIMIT 10;

-- Top 10 by CO2 emissions
SELECT v.make, v.model, f."CO2 Emissions(g/km)"
FROM vehicles v
JOIN fuel_emissions f ON v.vehicle_id = f.vehicle_id
ORDER BY f."CO2 Emissions(g/km)" DESC
LIMIT 10;
```

---

## Exploratory Data Analysis (SQL + Python)

The script produces:

- **Distribution of CO₂** (histogram + KDE)
- **CO₂ vs Engine Size** (scatter; colored by fuel type)
- **Average CO₂ by Vehicle Class** (bar chart)
- **Fuel Consumption vs CO₂** (scatter; colored by class)
- **Fuel Type counts** (bar chart)
- **CO₂ vs MPG** (scatter)
- **Correlation Heatmap** (engine size, cylinders, consumption metrics, MPG, CO₂)

---

## Machine Learning

Two regressors are trained/evaluated:

1. **Random Forest Regressor**
   - Features: `engine_size`, `cylinders`, `fuel_type (one-hot)`, `fuel_consumption_comb`, `fuel_efficiency_mpg`
   - Metrics printed: **MAE**, **RMSE**, **R²**
   - Plot: Actual vs Predicted CO₂

2. **Gradient Boosting Regressor**
   - Features: `engine_size`, `cylinders`, `fuel_consumption_comb`, `fuel_efficiency_mpg`
   - Outputs: **MAE**, **R²**, and **feature importances** (bar plot)

> All training/test splits use `train_test_split(..., test_size=0.2, random_state=42)` for reproducibility.

---

## Interactive Dashboard

Run the dashboard to explore relationships interactively:

```bash
python dashboard_app.py
# or if it is part of analysis.py, ensure it guards with:
# if __name__ == "__main__": app.run_server(debug=True, port=8060)
```

Then open: `http://localhost:8060`

**Features:**
- Choose X-axis numeric variable (e.g., engine size, fuel consumption metrics)
- Group/color by a categorical variable (Fuel Type / Vehicle Class / Make)
- Filter by CO₂ range via slider
- KPIs: Average CO₂, total vehicles in view, unique brands in view

---

## Results & Artifacts

- **SQLite DB**: `vehicle_emissions_normalised.db` (normalized schema for SQL EDA & modeling)  
- **Figures**: generated during analysis (distribution, scatter plots, heatmap, feature importances)  
- **Model Metrics**: printed in the console (MAE, RMSE, R²)



